{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_207896\\953422821.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  training_data = pd.read_csv('Data\\small.csv') # Load data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_207896\\953422821.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  training_data = pd.read_csv('Data\\small.csv') # Load data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions: \u001b[38;5;66;03m#loop through possible actions\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatching_training_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindiviual_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# get the reward\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindividual_next_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# get the next state\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[0;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:592\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    590\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m--> 592\u001b[0m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:283\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\flags.py:53\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[1;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrame, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Value Iteration\n",
    "\n",
    "training_data = pd.read_csv('Data\\small.csv') # Load data\n",
    "\n",
    "discount_factor = 0.95  # Initialize parameters\n",
    "iterations = 50 # do a fixed number of iterations as opposed to a bellman residual for simplicity\n",
    "\n",
    "unique_states = training_data['s'].drop_duplicates() # Store the states and actions into their own variables\n",
    "unique_actions = training_data['a'].drop_duplicates()\n",
    "states = training_data['s']\n",
    "actions = training_data['a']\n",
    "\n",
    "\n",
    "value_function = {} #Initalize a value function a dictionary of the an initzally 0 value function and the state it coorisponds to\n",
    "for state in unique_states: \n",
    "    value_function[state] = 0\n",
    "policy = {} # intialize policy list\n",
    "for state in unique_states:\n",
    "    policy[state] = np.random.choice(unique_actions) # select a radnom action from the actions that are available in the actions dataset\n",
    "\n",
    "for i in range(iterations): #iterate through the number of iterations we will perform\n",
    "    print(i)\n",
    "    for state in unique_states:\n",
    "        action = policy[state] # get the action that our current policy tells us to do, this will be random in the beginning\n",
    "        matching_training_data = training_data[(states == state) & (actions == action)] # find data that matches the current state and action pair\n",
    "        E_value = 0 # initalize the expected value\n",
    "        for j, row in matching_training_data.iterrows(): #iterate through the rows in the dataframe for which we have training data\n",
    "            indiviual_reward = row['r'] # get the reward\n",
    "            individual_next_state = row['sp'] # get the next state\n",
    "            E_value = E_value +  (indiviual_reward + (discount_factor * value_function[individual_next_state])) # Equation 7.16 in the textbook\n",
    "        if len(matching_training_data) > 0:\n",
    "            value_function[state] = E_value / matching_training_data.shape[0] # find the average for the value function if there were multiple matching training data points\n",
    "        else:\n",
    "           value_function[state] =  0\n",
    "        \n",
    "        highest_value_action = -1 # assign a default value to the best action each time the for loop below runs\n",
    "        best_value = -100000 # assign a low value to the inital best value\n",
    "        for action in actions: #loop through possible actions\n",
    "            action_value = 0\n",
    "            for j, row in matching_training_data.iterrows():\n",
    "                indiviual_reward = row['r'] # get the reward\n",
    "                individual_next_state = row['sp'] # get the next state\n",
    "                action_value = action_value + (indiviual_reward + (discount_factor * value_function[individual_next_state])) # determine the action value based on the updated value function\n",
    "            if len(matching_training_data) > 0: # if there is training data for this data point\n",
    "                action_value = action_value / matching_training_data.shape[0] # find the average for the action value if multiple iterations of training data was used\n",
    "            else: \n",
    "                action_value = 0\n",
    "            if action_value > best_value: # check if this action has higher value\n",
    "                best_value = action_value # store the new value if this is true\n",
    "                highest_value_action = action # store the new action if this is true\n",
    "        \n",
    "        policy[state] = highest_value_action # store the highest value action to the policy at the end of the iteration\n",
    "\n",
    "sorted_states = sorted(states)\n",
    "with open('small.policy', 'w') as file: # write the policy to the file \n",
    "    for s in sorted_states:\n",
    "        action = policy.get(s) \n",
    "        file.write(f\"{action}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_207896\\243051780.py:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  training_data = pd.read_csv('Data\\medium.csv') # Load data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_207896\\243051780.py:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  training_data = pd.read_csv('Data\\medium.csv') # Load data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m     individual_next_state \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# get the next state\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m action_value \u001b[38;5;241m+\u001b[39m (indiviual_reward \u001b[38;5;241m+\u001b[39m (discount_factor \u001b[38;5;241m*\u001b[39m value_function[individual_next_state])) \u001b[38;5;66;03m# determine the action value based on the updated value function\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatching_training_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# if there is training data for this data point\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m action_value \u001b[38;5;241m/\u001b[39m matching_training_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# find the average for the action value if multiple iterations of training data was used\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:1647\u001b[0m, in \u001b[0;36mDataFrame.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;124;03m    Returns length of info axis, but here we use the index.\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:909\u001b[0m, in \u001b[0;36mIndex.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    900\u001b[0m         c\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[: get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_dir_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m c\u001b[38;5;241m.\u001b[39misidentifier()\n\u001b[0;32m    903\u001b[0m     }\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# Array-Like Methods\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m# ndarray compat\u001b[39;00m\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    910\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Return the length of the Index.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Value Iteration\n",
    "\n",
    "training_data = pd.read_csv('Data\\medium.csv') # Load data\n",
    "\n",
    "discount_factor = 0.95  # Initialize parameters\n",
    "iterations = 50 # do a fixed number of iterations as opposed to a bellman residual for simplicity\n",
    "\n",
    "unique_states = training_data['s'].drop_duplicates() # Store the states and actions into their own variables\n",
    "unique_actions = training_data['a'].drop_duplicates()\n",
    "states = training_data['s']\n",
    "actions = training_data['a']\n",
    "\n",
    "value_function = {} #Initalize a value function a dictionary of the an initzally 0 value function and the state it coorisponds to\n",
    "for state in unique_states: \n",
    "    value_function[state] = 0\n",
    "policy = {} # intialize policy list\n",
    "for state in unique_states:\n",
    "    policy[state] = np.random.choice(unique_actions) # select a radnom action from the actions that are available in the actions dataset\n",
    "\n",
    "for i in range(iterations): #iterate through the number of iterations we will perform\n",
    "    print(i)\n",
    "    for state in unique_states:\n",
    "        action = policy[state] # get the action that our current policy tells us to do, this will be random in the beginning\n",
    "        matching_training_data = training_data[(states == state) & (actions == action)] # find data that matches the current state and action pair\n",
    "        E_value = 0 # initalize the expected value\n",
    "        for j, row in matching_training_data.iterrows(): #iterate through the rows in the dataframe for which we have training data\n",
    "            indiviual_reward = row['r'] # get the reward\n",
    "            individual_next_state = row['sp'] # get the next state\n",
    "            E_value = E_value +  (indiviual_reward + (discount_factor * value_function[individual_next_state])) # Equation 7.16 in the textbook\n",
    "        if len(matching_training_data) > 0:\n",
    "            value_function[state] = E_value / matching_training_data.shape[0] # find the average for the value function if there were multiple matching training data points\n",
    "        else:\n",
    "           value_function[state] =  0\n",
    "        \n",
    "        highest_value_action = -1 # assign a default value to the best action\n",
    "        best_value = -100000 # assign a low value to the inital best value\n",
    "        for action in actions: #loop through possible actions\n",
    "            action_value = 0\n",
    "            for j, row in matching_training_data.iterrows():\n",
    "                indiviual_reward = row['r'] # get the reward\n",
    "                individual_next_state = row['sp'] # get the next state\n",
    "                action_value = action_value + (indiviual_reward + (discount_factor * value_function[individual_next_state])) # determine the action value based on the updated value function\n",
    "            if len(matching_training_data) > 0: # if there is training data for this data point\n",
    "                action_value = action_value / matching_training_data.shape[0] # find the average for the action value if multiple iterations of training data was used\n",
    "            else: \n",
    "                action_value = 0\n",
    "            if action_value > best_value: # check if this action has higher value\n",
    "                best_value = action_value # store the new value if this is true\n",
    "                highest_value_action = action # store the new action if this is true\n",
    "        \n",
    "        policy[state] = highest_value_action # store the highest value action to the policy at the end of the iteration\n",
    "\n",
    "# Now that we have a policy for the training data, we need to extrapolate that to the eitire data set, we can do that doing nearest neighbor approximation\n",
    "\n",
    "\n",
    "possible_positions = np.arange(0, 500)\n",
    "possible_velocities = np.arange(0, 100)\n",
    "state_domain = 1 + possible_positions.reshape(-1, 1) + 500 * possible_velocities  #calcaulte the domain of all possible states\n",
    "state_domain_flat = state_domain.reshape(-1) # make the data 1D for use in a loop\n",
    "\n",
    "training_states = np.array(states).reshape(-1, 1) # Gather all the states used in training and convert into a 1D array for use in determination of actions\n",
    "training_actions = [] # initalize a tranining actions array\n",
    "for state in states: #iterate through training states\n",
    "    action = policy[state] # get the action from the policy\n",
    "    training_actions.append(action) #append the action to the policy\n",
    "training_actions = np.array(training_actions)\n",
    "\n",
    "distances = cdist(state_domain_flat.reshape(-1, 1), training_states, metric='euclidean') #use the nerarest neighbor function to determine the nearest neighbor \n",
    "nearest_neighbor = np.argmin(distances, axis=1)\n",
    "nearest_neighbor_actions = training_actions[nearest_neighbor] # store the nearest neighbor actions\n",
    "sorted_states = sorted(state_domain_flat) # sort states\n",
    "\n",
    "with open('medium.policy', 'w') as file: # Write the policy to a file\n",
    "    for s in sorted_states:\n",
    "        index = np.where(state_domain_flat == s)[0]\n",
    "        action = nearest_neighbor_actions[index[0]]\n",
    "        file.write(f\"{action}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Epochs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Epoch)\n\u001b[1;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#iterate through each known training data point\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#current state beginnign with 0\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# action state beginning with 0\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[0;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:574\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m [data]\n\u001b[0;32m    573\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    575\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(data, index)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Q-Learning\n",
    "\n",
    "training_data = pd.read_csv('data/large.csv')# Load data\n",
    "\n",
    "num_states = 302020\n",
    "num_actions = 9\n",
    "Q = np.zeros((num_states, num_actions)) # initalize the action-value lookup table with the number of states and number of possible actions at those states set to 0\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.1  # used a value of 0.1 after looking on ED\n",
    "discount_factor = 0.95  # known discount factor\n",
    "Epochs = 50  # This can be changed to increase the convergence of the Q learning\n",
    "\n",
    "for Epoch in range(Epochs):\n",
    "    print(Epoch)\n",
    "    for _, data_point in training_data.iterrows(): #iterate through each known training data point\n",
    "        state = data_point['s'] - 1 #current state beginnign with 0\n",
    "        action = data_point['a'] - 1 # action state beginning with 0\n",
    "        reward = data_point['r'] # reward \n",
    "        next_state = data_point['sp'] - 1  # next state beginnign with 0\n",
    "        max_a_prime_QSA = np.max(Q[next_state]) #use the action-value table to determine the maximum expected value at the next state, this will change over Epochs\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (reward + (discount_factor * max_a_prime_QSA - Q[state, action])) # Equation 17.10\n",
    "policy = np.argmax(Q, axis=1) + 1  #using the action-value matrix, extract the action that yields the highest expected value (argmax)\n",
    "\n",
    "with open('large.policy', 'w') as file: # Write the policy to a file\n",
    "    for p in  policy:\n",
    "        file.write(f\"{p}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
